% IEEE Paper Template - By Gemini (V3, Further Corrected)
\documentclass[conference]{IEEEtran}

%
% PACKAGES
%
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{microtype}
\usepackage{cite}
\usepackage{booktabs} % For professional-looking tables

% Configure hyperref for IEEE papers
% It's often recommended to load hyperref last
\usepackage[bookmarks=false, hidelinks]{hyperref}

%
% LISTINGS STYLE
%
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  breakatwhitespace=true,
  columns=fullflexible,
  keepspaces=true,
  numbers=left,
  numberstyle=\tiny,
  frame=single,
  framerule=0.5pt,
  rulecolor=\color{black!40},
  backgroundcolor=\color{black!2},
  showstringspaces=false,
  captionpos=b
}

%
% METADATA
%
\title{A Lightweight Deterministic Approach to Localization via Range-Based Odometry Correction}

\author{
    \IEEEauthorblockN{Danish Tapia}
    \IEEEauthorblockA{\textit{MIT Tech Team}\\
    danish.tapia017@gmail.com}
    \and
    \IEEEauthorblockN{Om Gunjal}
    \IEEEauthorblockA{\textit{Department or Group Name} \\
    \textit{MIT Tech Team}\\
    Cambridge, MA, USA \\
    om.gunjal@example.com}
}

% This command is used to get the desired BibTeX styles
\bstctlcite{IEEEexample:BSTcontrol}

\begin{document}
\maketitle

%
% CONTENT
%

\begin{abstract}
This paper presents a lightweight, deterministic localization algorithm for mobile robots operating within a known rectangular arena. The proposed method fuses wheel-encoder dead-reckoning and IMU-derived heading with measurements from a sparse set of range sensors to produce real-time pose estimates $(x, y, \theta)$. Localization is achieved by analytically predicting ray-wall intersections, validating measurements with a rule-based filter, and applying direct algebraic corrections to accumulated odometry offsets. This approach avoids the computational overhead of probabilistic filtering or iterative optimization. It is robust to transient occlusions and sensor dropouts, requires minimal arithmetic operations, and is well-suited for resource-constrained microcontrollers. We provide a detailed algorithmic breakdown, complexity analysis, and representative C-style code. A comprehensive experimental plan is proposed to validate the method's performance against standard EKF and particle filter baselines.
\end{abstract}

\begin{IEEEkeywords}
Localization, Range Sensors, Odometry, Sensor Fusion, Embedded Systems, Deterministic Algorithm, Mobile Robotics.
\end{IEEEkeywords}

\section{Introduction}
Reliable self-localization is a cornerstone of mobile robot autonomy. In many structured environments—such as arenas for robotic competitions, standardized warehouse aisles, or designated clean rooms—the robot's workspace is geometrically simple and known \textit{ad priori}. In these contexts, the primary challenge is not mapping, but rather maintaining an accurate pose estimate in the face of actuator and sensor noise, particularly odometric drift.

High-fidelity solutions like Simultaneous Localization and Mapping (SLAM) \cite{probrobot} or Iterative Closest Point (ICP) scan matching \cite{icp} are powerful but often require dense sensor data (e.g., from a LiDAR) and significant computational power, making them unsuitable for low-cost platforms. Probabilistic methods like Monte Carlo Localization (MCL) \cite{mcl} are more flexible but their performance is tied to the number of particles, which directly impacts CPU load.

This creates a distinct need for algorithms that are computationally frugal yet robust enough for real-time control. This is the gap our work addresses. We propose a deterministic localization algorithm that leverages the geometric constraints of a known rectangular environment to directly correct odometry errors. By using a sparse set of range sensors (e.g., time-of-flight or infrared), our method provides axis-by-axis corrections without resorting to complex state estimation.

The main contributions of this work are threefold:
\begin{itemize}
  \item A deterministic, analytic localization pipeline that computes closed-form pose corrections from as few as one valid range measurement.
  \item A robust, rule-based validation filter to handle real-world sensor issues like occlusions, saturation, and geometric ambiguities near corners.
  \item A detailed analysis of the algorithm's computational complexity and a reference implementation, demonstrating its suitability for embedded systems like STM32-class microcontrollers.
\end{itemize}

The paper is structured as follows: Section II reviews related work. Section III defines the system model and problem. Section IV details the proposed methodology. Section V analyzes its complexity. Section VI outlines an evaluation plan, and Section VII concludes with a discussion of limitations and future work.

% --- FIGURE PLACEHOLDER ---
\begin{figure}[t]
    \centering
    % \includegraphics[width=0.9\columnwidth]{figure_path/system_overview.pdf}
    \fbox{\parbox[c][1.8in][c]{0.8\columnwidth}{\centering \large System Overview Figure\\ \small (e.g., Robot in arena with sensors, walls, and coordinate frames)}}
    \caption{System overview showing the robot within the known rectangular arena of width $W$ and length $L$. The world frame $W$ is fixed to the arena, while the robot frame $R$ moves with the robot. The pose $(x, y, \theta)$ defines the transformation from $R$ to $W$.}
    \label{fig:system_overview}
\end{figure}
% --------------------------

\section{Related Work}
Localization for mobile robots is a mature field. Extended Kalman Filters (EKF) are a classic choice, fusing odometry and sensor measurements in a recursive Bayesian framework \cite{probrobot}. However, EKFs rely on linearization of the motion and measurement models, which can lead to divergence if nonlinearities are severe, as is common when a robot's sensors view different walls.

Particle filters, or MCL, overcome the linearization issue by representing the posterior distribution with a set of weighted samples \cite{mcl}. This allows them to handle multi-modal distributions and global localization problems. Their primary drawback is the computational cost, which scales with the number of particles required for accurate tracking.

Range-only localization has also been explored. O'Kane and LaValle established theoretical conditions for localization with minimal sensing \cite{okane}. More recent methods use optimization to solve the range-only localization problem, sometimes with formal guarantees \cite{rangeopt}, but typically target more powerful computing platforms. Our method differs by exploiting the specific geometry of a rectangular map to derive analytic constraints, trading generality for extreme computational efficiency.

\section{System Model and Problem Formulation}
\subsection{System Model}
We model the robot as a rigid body moving on a 2D plane.
\begin{itemize}
    \item \textbf{Pose:} The robot's pose in the world frame $W$ is $p = (x, y, \theta)^T$, where $(x, y)$ is the position of the robot's center and $\theta$ is its orientation.
    \item \textbf{Environment:} The environment is a rectangular arena of known dimensions, width $W$ and length $L$, with walls aligned with the world frame axes at $x=0, x=W, y=0, y=L$.
    \item \textbf{Sensors:} The robot is equipped with $N_s$ single-beam range sensors (up to 4 in our reference design). Each sensor $i$ is mounted at a fixed pose $(x_{tf,i}, y_{tf,i}, \phi_i)$ relative to the robot's frame $R$.
    \item \textbf{Motion:} The robot's motion is tracked via wheel encoders, which provide an estimate of the robot's displacement (odometry). An IMU provides an independent measurement of the global heading $\theta$.
\end{itemize}

\subsection{Problem Formulation}
The core problem is to estimate the robot's true pose $p$ by correcting the drift-prone odometry using the sparse range measurements. Each sensor $i$ provides a distance measurement $d_i$. The ideal measurement model is:
\[
    d_i = h_i(p) + \epsilon_i
\]
where $h_i(p)$ is the true distance from the sensor to the nearest wall along its measurement axis, and $\epsilon_i$ is zero-mean Gaussian noise. Our goal is to compute a corrective offset $(\delta_x, \delta_y)$ to the odometry-based pose estimate at a rate sufficient for real-time control.

\section{Proposed Methodology}
\subsection{Algorithmic Pipeline}
The algorithm is executed in a loop, typically synchronized with the arrival of new sensor data.
\begin{enumerate}
  \item \textbf{State Prediction:} A predicted pose is formed by adding a stored global offset to the current raw pose from wheel odometry.
  \item \textbf{Measurement Prediction:} For each sensor, the expected distance to a wall is calculated using the predicted pose and the known map geometry.
  \item \textbf{Measurement Validation:} Each real measurement is compared to its prediction and classified using a rule-based filter (see Fig. \ref{fig:validation_cases}).
  \item \textbf{Geometric Correction:} For each valid measurement, a direct, closed-form correction for either the $x$ or $y$ axis is computed.
  \item \textbf{Correction Fusion & Update:} Corrections from multiple sensors are fused, and the global odometry offset is updated.
\end{enumerate}

\subsection{Geometric Correction}
For a sensor $i$ with world pose $(x_s, y_s)$ and orientation $\theta_s$, the intersection point with a vertical wall at $X_{\text{wall}}$ can be used to find the true $x$-position of the robot's center. As illustrated in Fig. \ref{fig:geometry}, this is an algebraic calculation:
\[
    x = X_{\text{wall}} - (d_i \cos(\theta_s) + x_{tf,i})
\]
A similar equation holds for deriving the $y$-position from a horizontal wall. This provides a direct correction without iteration.

% --- FIGURE PLACEHOLDER ---
\begin{figure}[t]
    \centering
    % \includegraphics[width=0.9\columnwidth]{figure_path/geometry.pdf}
    \fbox{\parbox[c][1.8in][c]{0.8\columnwidth}{\centering \large Geometric Model Figure\\ \small (Illustration of ray-casting, wall intersection, and variable definitions)}}
    \caption{Geometric model for a single sensor. The sensor's ray, cast at angle $\theta_s$, intersects a vertical wall. The robot's true x-coordinate can be derived from the wall's known position $X_{\text{wall}}$ and the measured distance $d$.}
    \label{fig:geometry}
\end{figure}
% --------------------------

\subsection{Measurement Validation}
Robustness is achieved by filtering unreliable measurements. Each measurement is classified:
\begin{itemize}
  \item \texttt{VALID}: The measurement is consistent with the predicted distance and geometry.
  \item \texttt{BLOCKED}: Measured distance is significantly smaller than predicted, indicating an unmapped obstacle.
  \item \texttt{CORNER}: The sensor beam is aimed near a corner, where the intersecting wall is ambiguous.
  \item \texttt{ANGLE_INVALID}: The beam is nearly parallel to the wall, making the intersection point highly sensitive to noise.
  \item \texttt{SATURATED}: The sensor returns its maximum value, indicating no object was detected in range.
\end{itemize}
Only \texttt{VALID} measurements are passed to the correction stage.

% --- FIGURE PLACEHOLDER ---
\begin{figure}[t]
    \centering
    % \includegraphics[width=\columnwidth]{figure_path/validation_cases.pdf}
    \fbox{\parbox[c][1.5in][c]{0.9\columnwidth}{\centering \large Sensor Validation Cases Figure\\ \small (a) Valid Hit (b) Corner Case (c) Blocked Ray)}}
    \caption{Illustration of measurement validation logic. (a) A valid measurement where the measured distance matches the prediction. (b) A corner case where the ray's intersection is ambiguous. (c) A blocked ray where an obstacle causes a short reading. These cases are filtered deterministically.}
    \label{fig:validation_cases}
\end{figure}
% --------------------------

\subsection{Correction Fusion}
If multiple sensors provide corrections for the same axis (e.g., front and back sensors hitting parallel walls), their results are averaged to reduce noise. If only one valid measurement is available, its correction is used directly. The final correction is applied to a global offset variable, which continuously aligns the free-drifting odometry frame with the global world frame.

\section{Complexity and Implementation}
\subsection{Computational Complexity}
The algorithm's computational cost per cycle is exceptionally low. For $N_s$ sensors, the process involves:
\begin{itemize}
    \item $N_s$ sensor pose transformations (trigonometry, additions).
    \item $N_s$ ray-intersection calculations (4 wall checks each).
    \item $N_s$ validation checks (comparisons).
    \item A small, constant number of operations for fusion.
\end{itemize}
The total complexity is therefore $O(N_s)$, with no dependence on the map size (beyond the 4 walls) and no iterative loops. This makes it highly predictable and suitable for real-time execution.

\subsection{Implementation on Microcontrollers}
The algorithm was implemented in C for an STM32 microcontroller. Key considerations were the use of fixed-point arithmetic where possible and look-up tables for trigonometric functions to reduce floating-point load. The code snippet below shows the core ray-intersection logic.

\begin{lstlisting}[language=C,caption={Core ray-wall intersection logic}]
// Simplified logic for finding minimum intersection distance
float t_min = INFINITY;
// Check intersection with vertical walls (x=0, x=W)
if (fabs(dx) > EPS) {
    float t = (0.0 - sensor_x) / dx; // Left wall
    if (t > 0 && is_within_y_bounds(sensor_y + t*dy)) t_min = fmin(t_min, t);
    t = (W - sensor_x) / dx;   // Right wall
    if (t > 0 && is_within_y_bounds(sensor_y + t*dy)) t_min = fmin(t_min, t);
}
// Check intersection with horizontal walls (y=0, y=L)
if (fabs(dy) > EPS) {
    float t = (0.0 - sensor_y) / dy; // Bottom wall
    if (t > 0 && is_within_x_bounds(sensor_x + t*dx)) t_min = fmin(t_min, t);
    t = (L - sensor_y) / dy;   // Top wall
    if (t > 0 && is_within_x_bounds(sensor_x + t*dx)) t_min = fmin(t_min, t);
}
expected_dist[i] = t_min;
\end{lstlisting}

\section{Evaluation Plan}
We propose a comprehensive plan to validate our algorithm's performance against ground truth and standard baselines.

\subsubsection{Metrics}
The primary metrics will be the Root Mean Square Error (RMSE) of the estimated pose $(x, y, \theta)$ relative to a motion capture system, and the CPU load on the target microcontroller.

\subsubsection{Baselines}
We will compare our method against:
\begin{itemize}
  \item \textbf{Raw Odometry}: To quantify the baseline drift.
  \item \textbf{EKF}: A standard Extended Kalman Filter fusing odometry, heading, and range data.
  \item \textbf{MCL}: A lightweight particle filter with a particle count tuned to be computationally comparable to our method.
\end{itemize}

\subsubsection{Test Scenarios}
The robot will execute several trajectories, including:
\begin{itemize}
    \item A 4m x 2m rectangular path to test performance along straight lines and at corners.
    \item A figure-eight trajectory to test performance during continuous heading changes.
    \item A "kidnapped robot" test where the robot is manually moved to assess recovery.
    \item Obstacle tests where objects are temporarily placed to block sensors.
\end{itemize}
Expected results are summarized in the placeholder Table \ref{tab:results} and Fig. \ref{fig:trajectory}.

% --- TABLE PLACEHOLDER ---
\begin{table}[t]
\centering
\caption{Placeholder for Quantitative Results Summary}
\label{tab:results}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method} & \textbf{RMSE x (cm)} & \textbf{RMSE y (cm)} & \textbf{RMSE $\theta$ (deg)} & \textbf{CPU Load (\%)} \\ \midrule
Odometry Only & [Fill In] & [Fill In] & [Fill In] & N/A \\
Our Method    & [Fill In] & [Fill In] & [Fill In] & [Fill In] \\
EKF           & [Fill In] & [Fill In] & [Fill In] & [Fill In] \\
MCL (Light)   & [Fill In] & [Fill In] & [Fill In] & [Fill In] \\ \bottomrule
\end{tabular}
\end{table}
% --------------------------

% --- FIGURE PLACEHOLDER ---
\begin{figure}[t]
    \centering
    % \includegraphics[width=0.9\columnwidth]{figure_path/trajectory.pdf}
    \fbox{\parbox[c][1.8in][c]{0.8\columnwidth}{\centering \large Trajectory Plot Figure\\ \small (e.g., Ground truth vs. Odometry vs. Our Method)}}
    \caption{Example trajectory plot from a simulated run. The ground truth (ideal path) is compared against raw odometry (drifting) and the output of our proposed algorithm, which remains bounded.}
    \label{fig:trajectory}
\end{figure}
% --------------------------

\section{Discussion and Conclusion}
\subsection{Limitations and Future Work}
The primary limitation of our method is its reliance on a known, rectangular map. Extending the geometric engine to support general polygonal maps is a key direction for future work. This would involve replacing the analytic intersection formulas with a more general ray-casting algorithm, which may increase computational cost.

The algorithm is also sensitive to significant heading errors from the IMU. While the validation rules provide some robustness, large, uncorrected heading drift could lead to incorrect wall identifications. Future work could explore fusing corrections from multiple, non-parallel sensors to simultaneously solve for a small heading correction.

Finally, performance is critically dependent on accurate sensor calibration. An automated calibration routine to identify sensor poses and scale factors would greatly improve usability.

\subsection{Conclusion}
We have presented a deterministic, compAutationally lightweight localization algorithm for mobile robots in structured environments. By leveraging known geometry to create direct, algebraic corrections to odometry, our method provides a robust and efficient alternative to more complex probabilistic techniques. Its low, predictable computational footprint makes it ideal for embedded, real-time systems. The proposed evaluation will serve to quantify its performance benefits on physical hardware.

\section*{Acknowledgment}
% Acknowledge funding sources, colleagues, and other support here.
The authors thank the members of the MIT Tech Team for their support and the anonymous reviewers for their valuable feedback.

%
% BIBLIOGRAPHY
%
\bibliographystyle{IEEEtran} 
\bibliography{IEEEabrv,references}

\end{document}